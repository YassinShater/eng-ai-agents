# -*- coding: utf-8 -*-
"""Assignment 1b(Optimization algorithms for linear regression)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q8523aOK5NHhgx_SbbDhsuhDFMgOmf_N
"""

# Import necessary libraries
import torch
import numpy as np
import matplotlib.pyplot as plt

# Ensure reproducibility
torch.manual_seed(0)
np.random.seed(0)

# Define functions to create the toy dataset
def create_toy_data(func, sample_size, std, domain=[0, 1]):
    x = np.linspace(domain[0], domain[1], sample_size)
    np.random.shuffle(x)
    y = func(x) + np.random.normal(scale=std, size=x.shape)
    return x, y

def sinusoidal(x):
    return np.sin(2 * np.pi * x)

# Generate the dataset
x_train_np, y_train_np = create_toy_data(sinusoidal, 10, 0.25)
x_test_np = np.linspace(0, 1, 100)
y_test_np = sinusoidal(x_test_np)

# Plot the dataset
plt.figure(figsize=[10,8])
plt.scatter(x_train_np, y_train_np, facecolor="none", edgecolor="b", s=50, label="Training Data")
plt.plot(x_test_np, y_test_np, "-g", label="Target Function")
plt.legend()
plt.show()

from google.colab import drive
drive.mount('/content/drive')

# Convert numpy arrays to torch tensors
X_train = torch.tensor(x_train_np, dtype=torch.float32).unsqueeze(1)  # Shape: (N, 1)
y_train = torch.tensor(y_train_np, dtype=torch.float32).unsqueeze(1)  # Shape: (N, 1)

X_test = torch.tensor(x_test_np, dtype=torch.float32).unsqueeze(1)
y_test = torch.tensor(y_test_np, dtype=torch.float32).unsqueeze(1)

# Define the regularized MSE loss function
def compute_loss(X, y, weights, bias, lambda_reg=0.01):
    predictions = X @ weights + bias  # Linear model prediction: y_hat = XW + b
    mse_loss = torch.mean((predictions - y) ** 2)  # Mean Squared Error Loss
    l2_reg = lambda_reg * torch.sum(weights ** 2)  # L2 Regularization
    total_loss = mse_loss + l2_reg  # Total loss
    return total_loss

# Implement Stochastic Gradient Descent (SGD)
def sgd(X, y, weights, bias, learning_rate, epochs, lambda_reg):
    loss_history = []  # Store loss values over epochs
    n_samples = X.shape[0]
    for epoch in range(epochs):
        # Compute the loss
        loss = compute_loss(X, y, weights, bias, lambda_reg)
        # Compute gradients
        loss.backward()

        # Update weights and bias using SGD update rule
        with torch.no_grad():
            weights -= learning_rate * weights.grad
            bias -= learning_rate * bias.grad

        # Zero gradients for the next iteration
        weights.grad.zero_()
        bias.grad.zero_()

        # Store the loss value for this epoch
        loss_history.append(loss.item())

        # Optionally, print loss every 50 epochs
        if (epoch+1) % 50 == 0:
            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')
    return weights, bias, loss_history

# Hyperparameters
learning_rate = 0.1
epochs = 500
lambda_reg = 0.01  # Regularization strength

# Initialize weights and bias
weights_sgd = torch.randn((1, 1), requires_grad=True)
bias_sgd = torch.zeros(1, requires_grad=True)

# Train the model using SGD
weights_sgd, bias_sgd, loss_history_sgd = sgd(X_train, y_train, weights_sgd, bias_sgd, learning_rate, epochs, lambda_reg)

# Plot the Loss vs. Epochs for SGD
plt.figure(figsize=(10, 6))
plt.plot(loss_history_sgd, label="SGD Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss vs. Epochs for SGD")
plt.legend()
plt.grid(True)
plt.show()

# Plot the Final Hypothesis
with torch.no_grad():
    y_pred_sgd = X_test @ weights_sgd + bias_sgd

plt.figure(figsize=(10, 6))
plt.scatter(X_train.numpy(), y_train.numpy(), facecolor="none", edgecolor="b", s=50, label="Training Data")
plt.plot(X_test.numpy(), y_test.numpy(), "-g", label="Target Function")
plt.plot(X_test.numpy(), y_pred_sgd.numpy(), color='red', label='SGD Model')
plt.xlabel("Feature x")
plt.ylabel("Target y")
plt.title("Final Hypothesis Using SGD")
plt.legend()
plt.show()

# Implement Momentum-Based Optimization
def momentum(X, y, weights, bias, learning_rate, epochs, lambda_reg, momentum_coef):
    loss_history = []  # Store loss values over epochs

    # Initialize velocity terms for momentum updates
    velocity_w = torch.zeros_like(weights)
    velocity_b = torch.zeros_like(bias)

    for epoch in range(epochs):
        # Compute the loss
        loss = compute_loss(X, y, weights, bias, lambda_reg)
        # Compute gradients
        loss.backward()

        # Update weights and bias using Momentum update rule
        with torch.no_grad():
            velocity_w = momentum_coef * velocity_w + learning_rate * weights.grad
            weights -= velocity_w

            velocity_b = momentum_coef * velocity_b + learning_rate * bias.grad
            bias -= velocity_b

        # Zero gradients for the next iteration
        weights.grad.zero_()
        bias.grad.zero_()

        # Store the loss value for this epoch
        loss_history.append(loss.item())

        # Optionally, print loss every 50 epochs
        if (epoch+1) % 50 == 0:
            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')
    return weights, bias, loss_history

# Hyperparameters
learning_rate = 0.1
epochs = 500
lambda_reg = 0.01  # Regularization strength
momentum_coef = 0.9  # Momentum coefficient

# Initialize weights and bias
weights_momentum = torch.randn((1, 1), requires_grad=True)
bias_momentum = torch.zeros(1, requires_grad=True)

weights_momentum, bias_momentum, loss_history_momentum = momentum(
    X_train, y_train, weights_momentum, bias_momentum, learning_rate, epochs, lambda_reg, momentum_coef
)

# Plot the Loss vs. Epochs for Both Algorithms
plt.figure(figsize=(10, 6))
plt.plot(loss_history_sgd, label="SGD Loss")
plt.plot(loss_history_momentum, label="Momentum Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss vs. Epochs for SGD and Momentum")
plt.legend()
plt.grid(True)
plt.show()

# Plot the Final Hypotheses
with torch.no_grad():
    y_pred_sgd = X_test @ weights_sgd + bias_sgd
    y_pred_momentum = X_test @ weights_momentum + bias_momentum

plt.figure(figsize=(10, 6))
plt.scatter(X_train.numpy(), y_train.numpy(), facecolor="none", edgecolor="b", s=50, label="Training Data")
plt.plot(X_test.numpy(), y_test.numpy(), "-g", label="Target Function")
plt.plot(X_test.numpy(), y_pred_sgd.numpy(), color='red', label='SGD Model')
plt.plot(X_test.numpy(), y_pred_momentum.numpy(), color='purple', label='Momentum Model')
plt.xlabel("Feature x")
plt.ylabel("Target y")
plt.title("Final Hypotheses Using SGD and Momentum")
plt.legend()
plt.show()
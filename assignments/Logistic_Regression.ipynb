{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "fBro9vFDjMCv",
        "outputId": "10dcd3ca-0bdb-4962-e398-44894ee3df0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n",
            "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
            "0    -122.05     37.37                27.0       3885.0           661.0   \n",
            "1    -118.30     34.26                43.0       1510.0           310.0   \n",
            "2    -117.81     33.78                27.0       3589.0           507.0   \n",
            "3    -118.36     33.82                28.0         67.0            15.0   \n",
            "4    -119.67     36.33                19.0       1241.0           244.0   \n",
            "\n",
            "   population  households  median_income  median_house_value  \n",
            "0      1537.0       606.0         6.6085            344700.0  \n",
            "1       809.0       277.0         3.5990            176500.0  \n",
            "2      1484.0       495.0         5.7934            270500.0  \n",
            "3        49.0        11.0         6.1359            330000.0  \n",
            "4       850.0       237.0         2.9375             81700.0  \n",
            "\n",
            "Columns in the dataset:\n",
            "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
            "       'total_bedrooms', 'population', 'households', 'median_income',\n",
            "       'median_house_value'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Target column 'click' not found in the dataset columns.\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a9ef1ab7ec9b>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Target (CTR)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Target column '{target_column}' not found in the dataset columns.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Define Logistic Regression Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Target column 'click' not found in the dataset columns.\""
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Define the path to the uploaded file in the /sample_data directory\n",
        "file_path = '/content/sample_data/california_housing_test.csv'  # Replace 'your_file_name.csv' with your actual file name\n",
        "\n",
        "# Load the dataset from the specified path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows to ensure it loaded correctly\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Print the columns to see which ones are available\n",
        "print(\"\\nColumns in the dataset:\")\n",
        "print(data.columns)\n",
        "\n",
        "# Data Preprocessing (Modify according to your needs)\n",
        "# Adjust the column names based on what you see in the output of print(data.columns)\n",
        "# Drop only columns that exist in your dataset\n",
        "columns_to_drop = [col for col in ['id', 'hour'] if col in data.columns]\n",
        "df = data.drop(columns=columns_to_drop)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "target_column = 'click'  # Change this if your target column is named differently\n",
        "if target_column in df.columns:\n",
        "    X = torch.tensor(df.drop(columns=[target_column]).values, dtype=torch.float32)  # Features\n",
        "    y = torch.tensor(df[target_column].values, dtype=torch.float32).view(-1, 1)  # Target (CTR)\n",
        "else:\n",
        "    raise KeyError(f\"Target column '{target_column}' not found in the dataset columns.\")\n",
        "\n",
        "# Define Logistic Regression Model\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.sigmoid(self.linear(x))\n",
        "        return out\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = X.shape[1]\n",
        "model = LogisticRegressionModel(input_dim)\n",
        "\n",
        "# Define Loss function and Optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
        "\n",
        "# Training the model\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    y_pred = model(X)\n",
        "    loss = criterion(y_pred, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Generate predictions for evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_scores = model(X).numpy()\n",
        "\n",
        "# Calculate Precision-Recall Curve\n",
        "precision, recall, thresholds = precision_recall_curve(y.numpy(), y_scores)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title(f'Precision-Recall Curve (AUC = {pr_auc:.4f})')\n",
        "plt.show()\n",
        "\n",
        "# Explanation of the Precision-Recall Tradeoff\n",
        "print(\"\"\"\n",
        "Precision-Recall Tradeoff:\n",
        "- **Precision**: The ratio of true positive predictions to the total number of positive predictions.\n",
        "- **Recall**: The ratio of true positive predictions to the total number of actual positives.\n",
        "- A high precision-low recall scenario is useful when minimizing false positives is crucial.\n",
        "- A high recall-low precision scenario is better when minimizing false negatives is prioritized.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc2906c-dea0-4409-b1e6-fd92657bfba1",
        "id": "ZD0u-6Ekokq2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in the ZIP archive: ['sampleSubmission.gz', 'test.gz', 'train.gz']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import zipfile  # For handling ZIP files\n",
        "\n",
        "# Define the path to the uploaded ZIP file\n",
        "zip_file_path = '/content/avazu-ctr-prediction.zip'\n",
        "\n",
        "# Step 1: Extract and display file names in the ZIP archive\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    file_list = zip_ref.namelist()\n",
        "    print(f\"Files in the ZIP archive: {file_list}\")\n",
        "\n",
        "    # Step 2: Specify the file to read inside the ZIP archive\n",
        "    # Choose 'train.gz' as an example (change based on your needs)\n",
        "    file_to_read = 'train.gz'\n",
        "\n",
        "    # Step 3: Read the specific file using pandas while keeping the ZIP file reference open\n",
        "    with zip_ref.open(file_to_read) as extracted_file:\n",
        "        data = pd.read_csv(extracted_file, compression='gzip', header=0, sep=',', quotechar='\"', engine='python')\n",
        "\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(data.head())\n",
        "print(\"\\nColumns in the dataset:\")\n",
        "print(data.columns)\n",
        "\n",
        "# Data Preprocessing (Modify according to your needs)\n",
        "columns_to_drop = [col for col in ['id', 'hour'] if col in data.columns]\n",
        "df = data.drop(columns=columns_to_drop)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "target_column = 'click'  # Change this if your target column is named differently\n",
        "if target_column in df.columns:\n",
        "    X = torch.tensor(df.drop(columns=[target_column]).values, dtype=torch.float32)  # Features\n",
        "    y = torch.tensor(df[target_column].values, dtype=torch.float32).view(-1, 1)  # Target (CTR)\n",
        "else:\n",
        "    raise KeyError(f\"Target column '{target_column}' not found in the dataset columns.\")\n",
        "\n",
        "# Define Logistic Regression Model\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.sigmoid(self.linear(x))\n",
        "        return out\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = X.shape[1]\n",
        "model = LogisticRegressionModel(input_dim)\n",
        "\n",
        "# Define Loss function and Optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
        "\n",
        "# Training the model\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    y_pred = model(X)\n",
        "    loss = criterion(y_pred, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Generate predictions for evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_scores = model(X).numpy()\n",
        "\n",
        "# Calculate Precision-Recall Curve\n",
        "precision, recall, thresholds = precision_recall_curve(y.numpy(), y_scores)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title(f'Precision-Recall Curve (AUC = {pr_auc:.4f})')\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "Precision-Recall Tradeoff:\n",
        "- **Precision**: The ratio of true positive predictions to the total number of positive predictions.\n",
        "- **Recall**: The ratio of true positive predictions to the total number of actual positives.\n",
        "- A high precision-low recall scenario is useful when minimizing false positives is crucial.\n",
        "- A high recall-low precision scenario is better when minimizing false negatives is prioritized.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lNn73ABXkXdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zULCtBykkOnS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}